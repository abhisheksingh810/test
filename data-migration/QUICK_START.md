# Quick Start: Parallel Migration

## What Was Implemented

âœ… **Worker Pool Pattern** with 5 worker threads for true parallelization  
âœ… **Dynamic Work Distribution** - Workers pull from shared queue (most recent first)  
âœ… **Race Condition Prevention** - Single writer pattern, per-worker DB connections  
âœ… **Connection Pooling** - 3 connections per worker (15 total)  
âœ… **Progress Tracking** - Real-time updates every 10 seconds  
âœ… **Efficient CSV Writes** - Batch writes every 5 minutes  
âœ… **Error Handling** - Independent worker errors with file cleanup  
âœ… **True Parallelization** - Worker threads on separate CPU cores  
âœ… **Auto-compilation** - Worker script compiled with esbuild at startup  

## Expected Performance

| Metric | Sequential | Parallel (5 workers) |
|--------|-----------|---------------------|
| Speed | 3-5 sub/s | 12-20 sub/s |
| Time for 30k | 2-3 hours | 25-40 minutes |
| CPU Cores Used | 1 | 5 |
| Speedup | 1x | 3-5x |

## Run the Migration

```bash
# Make sure your .env file is configured
npm run migrate:rogo:parallel
```

## What You'll See

```
ğŸš€ Starting parallel Rogo submissions migration with worker pool pattern...
âš™ï¸  Configuration: 5 worker threads

ğŸ”§ Compiling worker script...
âœ… Worker script compiled successfully

ğŸ“‚ Loading submission_files.csv...
âœ… Loaded 30000 submission rows from CSV

ğŸ“‚ Loading attempt_number_mapping.csv...
âœ… Loaded 30000 attempt mappings

ğŸ“‚ Loading assessments from database...
âœ… Loaded 150 assessments from database

ğŸ“‚ Loading malpractice levels from database...
âœ… Loaded 4 malpractice levels from database

ğŸ“‚ Finding system user for migration operations...
âœ… Using system user ID: abc-123-def

ğŸ”§ Spawning 5 worker threads...
âœ… All 5 workers spawned and ready

ğŸš€ Starting migration...

================================================================================
ğŸ“Š Progress Update
================================================================================
âœ… Successful: 1450 | âŒ Failed: 15 | â­ï¸  Skipped: 235
ğŸ“ Total Processed: 1700/30000 (5.7%)
â±ï¸  Elapsed: 120.5s | Rate: 14.11 submissions/s | ETA: 2006s

ğŸ”§ Worker Statistics:
   Worker 0: 340 processed (330 âœ…, 10 âŒ)
   Worker 1: 342 processed (338 âœ…, 4 âŒ)
   Worker 2: 338 processed (335 âœ…, 3 âŒ)
   Worker 3: 340 processed (332 âœ…, 8 âŒ)
   Worker 4: 340 processed (338 âœ…, 2 âŒ)
================================================================================
```

## Files Created

### Input Files (Required)
- `submission_files.csv` - Main submission data
- `attempt_number_mapping.csv` - Attempt number mapping

### Output Files (Generated)
- `migrated.csv` - Successfully migrated submissions
- `submissions_failed_migration.csv` - Failed submissions with errors

## Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Main Thread                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ Work Queue  â”‚â”€â–¶â”‚ Distributor  â”‚â”€â–¶â”‚ CSV Writer   â”‚      â”‚
â”‚  â”‚ (Reversed)  â”‚  â”‚              â”‚  â”‚ (Periodic)   â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â”‚         â”‚                                     â–²              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚ Work Request                        â”‚ Results
          â–¼                                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Worker Pool                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Worker 0  â”‚  Worker 1  â”‚  Worker 2  â”‚  Worker 3  â”‚ Worker 4 â”‚
â”‚            â”‚            â”‚            â”‚            â”‚          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚Postgresâ”‚ â”‚ â”‚Postgresâ”‚ â”‚ â”‚Postgresâ”‚ â”‚ â”‚Postgresâ”‚ â”‚â”‚Postgresâ”‚â”‚
â”‚ â”‚Pool(3) â”‚ â”‚ â”‚Pool(3) â”‚ â”‚ â”‚Pool(3) â”‚ â”‚ â”‚Pool(3) â”‚ â”‚â”‚Pool(3) â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚            â”‚            â”‚            â”‚            â”‚          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ MSSQL  â”‚ â”‚ â”‚ MSSQL  â”‚ â”‚ â”‚ MSSQL  â”‚ â”‚ â”‚ MSSQL  â”‚ â”‚â”‚ MSSQL  â”‚â”‚
â”‚ â”‚Pool(3) â”‚ â”‚ â”‚Pool(3) â”‚ â”‚ â”‚Pool(3) â”‚ â”‚ â”‚Pool(3) â”‚ â”‚â”‚Pool(3) â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚            â”‚            â”‚            â”‚            â”‚          â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚ â”‚ Azure  â”‚ â”‚ â”‚ Azure  â”‚ â”‚ â”‚ Azure  â”‚ â”‚ â”‚ Azure  â”‚ â”‚â”‚ Azure  â”‚â”‚
â”‚ â”‚ Blob   â”‚ â”‚ â”‚ Blob   â”‚ â”‚ â”‚ Blob   â”‚ â”‚ â”‚ Blob   â”‚ â”‚â”‚ Blob   â”‚â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Message Flow

```
Main Thread                  Worker Thread
    â”‚                             â”‚
    â”‚â—€â”€â”€â”€ WORK_REQUEST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                             â”‚
    â”œâ”€â”€â”€â”€ WORK (csvRow) â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
    â”‚                             â”‚
    â”‚                             â”œâ”€ Process Submission
    â”‚                             â”œâ”€ Query Warehouse
    â”‚                             â”œâ”€ Upload to Azure
    â”‚                             â”œâ”€ Insert to Database
    â”‚                             â”‚
    â”‚â—€â”€â”€â”€ PROGRESS (optional) â”€â”€â”€â”€â”¤
    â”‚                             â”‚
    â”‚â—€â”€â”€â”€ WORK_RESULT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                             â”‚
    â”œâ”€ Aggregate Results          â”‚
    â”œâ”€ Write to CSV (periodic)    â”‚
    â”‚                             â”‚
    â”‚â—€â”€â”€â”€ WORK_REQUEST â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚                             â”‚
    â””â”€â”€â”€â”€ WORK (csvRow) â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚
```

## Race Condition Solutions

### Problem 1: CSV File Writes
**Solution**: Single writer pattern - only main thread writes to files

### Problem 2: Database Connections
**Solution**: Each worker has independent connection pools

### Problem 3: Azure Blob Uploads
**Solution**: Each worker has separate Azure service instance

### Problem 4: Duplicate Detection
**Solution**: Database query + transaction ensures atomicity

### Problem 5: Malpractice Enforcement Conflicts
**Solution**: Upsert logic within transaction (UPDATE or INSERT)

## Configuration

Edit `migrate-rogo-submissions-parallel.ts`:

```typescript
const NUM_WORKERS = 5;                      // Number of worker threads
const PROGRESS_UPDATE_INTERVAL = 10000;     // Progress updates (ms)
const BATCH_WRITE_INTERVAL = 300000;        // CSV write interval (ms)
```

Edit `migration-worker.ts`:

```typescript
warehouseDb.config.pool = {
  max: 3,                                 // Connections per worker
  min: 1,
  idleTimeoutMillis: 30000,
};
```

## Monitoring

### Real-time Progress
Updates every 10 seconds showing:
- Success/Failed/Skipped counts
- Processing rate (submissions/second)
- Estimated time remaining
- Per-worker statistics

### Output Files
- Check `migrated.csv` for successful migrations
- Check `submissions_failed_migration.csv` for errors

### Database Monitoring
```sql
-- Check latest migrations
SELECT * FROM assignment_submissions 
WHERE lti_launch_id LIKE 'rogo_%' 
ORDER BY created_at DESC 
LIMIT 100;

-- Count migrations
SELECT COUNT(*) FROM assignment_submissions 
WHERE lti_launch_id LIKE 'rogo_%';
```

## Troubleshooting

### Workers Not Starting
- Check Worker Threads support: `node --version` (v12.11.0+)
- Check TypeScript config: `ts-node` with `--esm` flag
- Check permissions on worker script file

### Slow Performance
- Check database connection latency
- Check Azure SQL Server throttling
- Monitor CPU usage (should be high across multiple cores)
- Consider reducing workers if connection-limited

### High Memory Usage
- Reduce `BATCH_WRITE_INTERVAL` to flush more frequently
- Monitor with: `node --max-old-space-size=8192` (increase if needed)

### Connection Pool Exhausted
- Reduce `NUM_WORKERS`
- Increase pool size per worker
- Check Azure SQL connection limits

## Comparison to Sequential

### Use Parallel When:
âœ… Large dataset (10k+ submissions)  
âœ… Need faster completion time  
âœ… Have multiple CPU cores available  
âœ… Database can handle concurrent connections  

### Use Sequential When:
âœ… Small dataset (< 1k submissions)  
âœ… Debugging specific issues  
âœ… Limited database connections  
âœ… Single-core environment  

## Safety Features

1. âœ… **Duplicate Prevention**: Database checks before inserting
2. âœ… **File Cleanup**: Azure blobs deleted on error
3. âœ… **Transaction Safety**: Rollback on error
4. âœ… **Graceful Shutdown**: Workers closed cleanly
5. âœ… **Progress Persistence**: Can resume from CSV
6. âœ… **Error Logging**: All errors tracked with reasons

## Next Steps After Migration

1. **Verify Results**
   ```bash
   npm run verify:migration
   ```

2. **Check Failed Migrations**
   - Review `submissions_failed_migration.csv`
   - Fix issues and rerun if needed

3. **Monitor Database**
   - Check for data consistency
   - Verify relationships (submissions â†’ files â†’ grades)

4. **Clean Up**
   - Archive CSV files
   - Document any manual fixes needed
   - Update team on migration status

## Support

For detailed documentation, see:
- `PARALLEL_MIGRATION_README.md` - Full architecture and design
- `migrate-rogo-submissions-parallel.ts` - Main coordinator code
- `migration-worker.ts` - Worker thread implementation

